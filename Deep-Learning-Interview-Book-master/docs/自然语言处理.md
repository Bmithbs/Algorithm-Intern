[TOC]

# 自然语言处理

## 列出几种文本特征提取算法

答：文档频率、信息增益、互信息、X^2统计、TF-IDF

## RNN

### 基本原理

- [ ] TODO

### RNN 常见的几种设计模式是怎样的？

- [ ] TODO

### RNN 为什么会梯度消失？

- [ ] TODO

### RNN 为什么会梯度爆炸？

- [ ] TODO

### RNN中为什么要采用tanh而不是ReLu作为激活函数？

- [ ] TODO

### RNN和CNN对比，RNN对文本的时间序列的优点。

- [ ] TODO

## LSTM

### LSTM 基本原理

- [ ] TODO

### LSTM 怎么能解决梯度消失问题？

- [ ] TODO

### LSTM 用来解决RNN的什么问题？

- [ ] TODO

### LSTM 和 GRU 区别

- [ ] TODO

### RNN、LSTM 和 GRU公式与结构图

- [ ] TODO

## 深度学习如何提取query特征？

- [ ] TODO

### 如何比较文本的相似度？

- 协同过滤相似度
- 余弦相似度
- tf-idf相似度
- 深度学习词向量

## 如何利用深度学习计算语义相似度？

- [ ] TODO

## word2vec

### word2vec 基本原理

- [ ] TODO

### 使用哈夫曼树的原因是什么？

- [ ] TODO

### word2vec的输入输出层是什么样的？

- [ ] TODO

### word2vec 里面的层次索引

- [ ] TODO

### word2vec如何训练的？

- [ ] TODO

### 如何判断 word2vec 的效果好坏？

- [ ] TODO

### 介绍一下 Word2vec，CBOW和Skip-gram的区别是什么？

- [ ] TODO

### word2vec 和 tf-idf 相似度计算时的区别？

- [ ] TODO

### word2vec 和 NNLM 对比有什么区别？（word2vec vs NNLM）

- [ ] TODO

### word2vec 负采样有什么作用？

负采样这个点引入word2vec非常巧妙，两个作用，1.加速了模型计算，2.保证了模型训练的效果，一个是模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

### word2vec 和 fastText对比有什么区别？（word2vec vs fastText）

- [ ] TODO

## GRU 和 LSTM、RNN的区别是什么？

- [ ] TODO

## Sentence Embedding

- [ ] TODO

## SeqSeq

- [ ] TODO

### seq2seq-attention原理和公式

- [ ] TODO

## Transformer 

### 基本原理

- [ ] TODO

### transformer 各部分怎么用？Q K V怎么计算；Attention怎么用？

- [ ] TODO

## 详细说一下 transformer 的 encoder 过程

- [ ] TODO

## 如何做文本摘要？

- [ ] TODO

## CTC loss公式推导

- [ ] TODO

## ELMO

- [ ] TODO

## BERT

### 基本原理

- [ ] TODO

### elmo、GPT、bert三者之间有什么区别？（elmo vs GPT vs bert）

- [ ] TODO

## XLNet

### 基本原理

- [ ] TODO

## 生成式问答解决生成句子多样性的方法

- [ ] TODO

## 怎么评价生成效果的好坏？

- [ ] TODO

## 参考资料

- [2018-暑期实习生-自然语言处理算法岗-面试题](<https://blog.csdn.net/qq_28031525/article/details/80028055>)
- [NLP Interview Questions](https://medium.com/modern-nlp/nlp-interview-questions-f062040f32f7)  [百度云链接](https://pan.baidu.com/s/1fY8HXiswGA1rbCnuGG5TXA)  提取码：h9k8 
- https://github.com/songyingxin/NLPer-Interview
