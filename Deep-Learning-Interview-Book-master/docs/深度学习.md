[TOC]

# 深度学习

## 神经网络中的 Epoch、Iteration、Batchsize

神经网络中 epoch 与 iteration 是不相等的

- batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用 SGD 训练，即每次训练在训练集中取 batchsize 个样本训练；

- iteration：中文翻译为迭代，1 个 iteration 等于使用 batchsize 个样本训练一次；一个迭代 = 一个正向通过+一个反向通过

- epoch：迭代次数，1 个 epoch 等于使用训练集中的全部样本训练一次；一个 epoch = 所有训练样本的一个正向传递和一个反向传递

举个例子，训练集有 1000 个样本，batchsize=10，那么：训练完整个样本集需要：100 次 iteration，1 次 epoch。

![img](https://gss0.baidu.com/-vo3dSag_xI4khGko9WTAnF6hhy/zhidao/wh%3D600%2C800/sign=36204981f1039245a1e0e909b7a488fa/e61190ef76c6a7ef3bf5176af0faaf51f2de66af.jpg)

**参考资料**

- [神经网络中的 Epoch、Iteration、Batchsize](https://zhuanlan.zhihu.com/p/67414365)
- [神经网络中 epoch 与 iteration 相等吗](https://zhidao.baidu.com/question/716300338908227765.html)

## 反向传播（BP）

- [ ] TODO

**参考资料**

- [一文搞懂反向传播算法](https://www.jianshu.com/p/964345dddb70)

## CNN 本质和优势

局部卷积（提取局部特征）

权值共享（降低训练难度）

Pooling（降维，将低层次组合为高层次的特征）

多层次结构

## 鞍点的定义和特点？

- [ ] TODO

## 神经网络数据预处理方法有哪些？

- [ ] TODO

## 神经网络怎样进行参数初始化？

- [ ] TODO

## 卷积

- [ ] TODO

**参考资料**

- [Feature Extraction Using Convolution](http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)
- [convolution](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html)

- [理解图像卷积操作的意义](https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&fps=1)

- [关于深度学习中卷积核操作](https://www.cnblogs.com/Yu-FeiFei/p/6800519.html)

### 卷积的反向传播过程

- [ ] TODO

**参考资料**

- [Notes on Convolutional Neural Network](http://cogprints.org/5869/1/cnn_tutorial.pdf)
- [Deep Learning 论文笔记之（四）CNN 卷积神经网络推导和实现](https://blog.csdn.net/zouxy09/article/details/9993371)

- [反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)

- [Deep learning：五十一 (CNN 的反向求导及练习）](https://www.cnblogs.com/tornadomeet/p/3468450.html)

- [卷积神经网络 (CNN) 反向传播算法](https://www.cnblogs.com/pinard/p/6494810.html)

- [卷积神经网络 (CNN) 反向传播算法公式详细推导](https://blog.csdn.net/walegahaha/article/details/51945421)

- [全连接神经网络中反向传播算法数学推导](https://zhuanlan.zhihu.com/p/61863634)

- [卷积神经网络 (CNN) 反向传播算法推导](https://zhuanlan.zhihu.com/p/61898234)

## CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？

对于一个卷积层，假设其大小为  （其中 c 为#input channel, n 为#output channel），输出的 feature map 尺寸为  ，则该卷积层的

- paras =![](https://www.zhihu.com/equation?tex=n+%5Ctimes+%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29)

- FLOPs = ![](https://www.zhihu.com/equation?tex=H%27+%5Ctimes+W%27+%5Ctimes+n+%5Ctimes%28h+%5Ctimes+w+%5Ctimes+c+%2B+1%29)

- [ ] TODO

**参考资料**

- [CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？](https://www.zhihu.com/question/65305385/answer/256845252)
- [CNN 中 parameters 和 FLOPs 计算](https://blog.csdn.net/sinat_34460960/article/details/84779219)
- [FLOPS 理解](https://blog.csdn.net/smallhujiu/article/details/80876875)
- [PyTorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)

## 池化（Pooling）

**平均池化（Mean Pooling）**

mean pooling 的前向传播就是把一个 patch 中的值求取平均来做 pooling，那么反向传播的过程也就是把某个元素的梯度等分为 n 份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，还是比较理解的，图示如下 

![](https://img-blog.csdn.net/20170615205352655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**最大池化（Max Pooling）**

max pooling 也要满足梯度之和不变的原则，max pooling 的前向传播是把 patch 中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为 0。所以 max pooling 操作和 mean pooling 操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是 max id，这个可以看 caffe 源码的 pooling_layer.cpp，下面是 caffe 框架 max pooling 部分的源码

```python

// If max pooling, we will initialize the vector index part.

if (this->layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX && top.size() == 1)

{

    max_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,pooled_width_);

  }

```

![](https://img-blog.csdn.net/20170615211413093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

**参考资料**

- [如何理解 CNN 中的池化？](https://zhuanlan.zhihu.com/p/35769417)
- [深度学习笔记（3）——CNN 中一些特殊环节的反向传播](https://blog.csdn.net/qq_21190081/article/details/72871704)

### 池化层怎么接收后面传过来的损失？

RF 是感受野。N_RF 和 RF 有点像，N 代表 neighbour，指的是第 n 层的 a feature 在 n-1 层的 RF，记住 N_RF 只是一个中间变量，不要和 RF 混淆。 stride 是步长，ksize 是卷积核大小。

(N-1)_RF = f(N_RF, stride, kernel) = (N_RF - 1) * stride + kernel

第一层是 3*3，第二层是 7*7
![黄色 feature map 对应的感受野是 7*7 大小](https://pic3.zhimg.com/80/v2-a71f153d40ac8281a39bc0f39d1158ba_hd.jpg)

黄色 feature map 对应的感受野是 7*7 大小

### 平均池化（average pooling）

- [ ] TODO

### 最大池化（max pooling）

- [ ] TODO

## 感受野

感受野（$Receptive$  $Field$）的定义是卷积神经网络每一层输出的特征图（$feature$ $map$）上的像素点在**原始输入图片**上映射的区域大小。再通俗点的解释是，特征图上的一个点对应**原始输入图片**上的区域，如下图所示。

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803232626.png)

### 感受野的作用

(1) 一般 $task$ 要求感受野越大越好，如图像分类中最后卷积层的感受野要大于输入图像，网络深度越深感受野越大性能越好； 

(2) 密集预测$task$要求输出像素的感受野足够的大，确保做出决策时没有忽略重要信息，一般也是越深越好； 

(3) 目标检测$task$中设置$anchor$要严格对应感受野，$anchor$太大或偏离感受野都会严重影响检测性能。

### 感受野计算

在计算感受野时有下面几点需要说明：

(1) 第一层卷积层的输出特征图像素的感受野的大小等于卷积核的大小。

(2) 深层卷积层的感受野大小和它之前所有层的滤波器大小和步长有关系。

(3) 计算感受野大小时，忽略了图像边缘的影响，即不考虑 padding 的大小。

下面给出计算感受野大小的计算公式：

$$
RF_{l+1} =  (RF_{l}-1)*stide + kernel
$$

其中$RF_{l+1}$为当前特征图对应的感受野的大小，也就是要计算的目标感受野，$RF_{l}$为上一层特征图对应的感受野大小，$f_{l+1}$为当前卷积层卷积核的大小，累乘项$strides$表示当前卷积层之前所有卷积层的步长乘积。

举一个例子，7 * 7 的输入图经过三层卷积核为 3 * 3 的卷积操作后得到$Out3$的感受野为 7 * 7，也就是$Out3$中的值是由$Input$所有区域的值经过卷积计算得到，其中卷积核 ($filter$) 的步长 ($stride$) 为 1、$padding$为 0，如下图所示：

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803233301.png)

以上面举的$sample$为例：

$Out1$层由于是第一层卷积输出，即其感受野等于其卷积核的大小，**即第一层卷积层输出的特征图的感受野为 3**，$RF1$=3；

$Out2$层的感受野$RF2$ = 3 + (3 - 1) * 1 = 5，**即第二层卷积层输出的特征图的感受野为 5**；

$Out3$层的感受野$RF3$ = 3 + (5 - 1) * 1 = 7，**即第三层卷积层输出的特征图的感受野为 7**；

**参考资料**

- [卷积神经网络物体检测之感受野大小计算](https://www.cnblogs.com/objectDetect/p/5947169.html)

- [如何计算感受野 (Receptive Field)——原理](https://zhuanlan.zhihu.com/p/31004121)
- [Computing Receptive Fields of Convolutional Neural Networks](https://distill.pub/2019/computing-receptive-fields/)

### 卷积神经网络的感受野

- [ ] TODO

**参考资料**

- [卷积神经网络的感受野](https://zhuanlan.zhihu.com/p/44106492)

## 权重初始化方法

- [ ] TODO

### Xavier

- [ ] TODO

## 正则化方法

1. [参数范数惩罚](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#1)
2. [L2 参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#2)
3. [L1 参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#3)
4. [L1 正则化和 L2 正则化的区别](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#4)
5.  [数据集增强](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#5)
6.  [噪音的鲁棒性](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#6)
7. [向输出目标注入噪声](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#7)
8.  [半监督学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#8)
9.  [多任务学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#9)
10. [提前终止](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#10)
11. [参数绑定和共享](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#11)
12. [稀疏表示](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#12)
13. [集成化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#13)

**参考资料**

- [ ] [正则化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin)
- [ ] [你够全面了解 L1 与 L2 正则吗？](https://mp.weixin.qq.com/s?__biz=MzkzNDIxMzE1NQ==&mid=2247486609&idx=1&sn=76fc19df55a2d7f605b8203ccd5f101c&chksm=c241efddf53666cb70ebb3b44a40154778c94c2a2c2fda74418ce994805b99bd2ee644becf68&scene=178&cur_album_id=1860258784426672132#rd)

## Batch Normalization（BN）

### BN 原理

> 为了解决 ICS 的问题，（但是有人求证过了，ICS 的问题并不能完全解决），BN 启发于白化操作。

因为深层神经网络在做非线性变换前的激活输入值（就是那个 x=WU+B，U 是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的。

BN 就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为 0 方差为 1 的标准正态分布 ，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，

上面记不住没关系，记住“保持数据同分布”即可

**参考资料**

- [Batch Normalization 原理与实战](https://zhuanlan.zhihu.com/p/34879333)

### 手写 BN

的简单计算步骤为：

- 沿着通道计算每个 batch 的均值$\mu=\frac{1}{m} \sum_{i=1}^{m} x_{i}$。

- 沿着通道计算每个$batch$的方差$\delta^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}$。

- 对 x 做归一化，![image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803224334.png)

- 加入缩放和平移变量$\gamma$和$\beta$ , 归一化后的值，$y_{i} \leftarrow \gamma \widehat{x}_{i}+\beta$

$BN$适用于判别模型中，比如图片分类模型。因为$BN$注重对每个$batch$进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是$BN$对$batchsize$的大小比较敏感，由于每次计算均值和方差是在一个$batch$上，所以如果$batchsize$太小，则计算的均值、方差不足以代表整个数据分布。

### BN 的作用？

- $BN$加快网络的训练与收敛的速度

  在深度神经网络中中，如果每层的数据分布都不一样的话，将会导致网络非常难收敛和训练。如果把每层的数据都在转换在均值为零，方差为 1 的状态下，这样每层数据的分布都是一样的训练会比较容易收敛。

- 控制梯度爆炸防止梯度消失

  以$sigmoid$函数为例，$sigmoid$函数使得输出在$[0,1]$之间，实际上当 输入过大或者过小，经过 sigmoid 函数后输出范围就会变得很小，而且反向传播时的梯度也会非常小，从而导致梯度消失，同时也会导致网络学习速率过慢；同时由于网络的前端比后端求梯度需要进行更多次的求导运算，最终会出现网络后端一直学习，而前端几乎不学习的情况。Batch Normalization (BN) 通常被添加在每一个全连接和激励函数之间，使数据在进入激活函数之前集中分布在 0 值附近，大部分激活函数输入在 0 周围时输出会有加大变化。

  同样，使用了$BN$之后，可以使得权值不会很大，不会有梯度爆炸的问题。

- 防止过拟合

  在网络的训练中，BN 的使用使得一个$minibatch$中所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个$batch$的其他样本，而每次网络都是随机取$batch$，比较多样，可以在一定程度上避免了过拟合。

### BN 有哪些参数？

γ和β两个尺度变化系数

### BN 的反向传播推导

![image](https://user-images.githubusercontent.com/47493620/118059082-39d1a500-b3c2-11eb-80f8-75f2bf677451.png)

  下面来一个背诵版本：

![image](https://user-images.githubusercontent.com/47493620/118059220-9208a700-b3c2-11eb-841f-73781fa93342.png)

### BN 在训练和测试的区别？

![image-20210803225110049](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803225111.png)

- [推理时 cnn bn 折叠；基于 KWS 项目](https://blog.csdn.net/weixin_37598106/article/details/106825799)

### BN 是放在激活前面还是后面？

一般是放在激活层前面，但是自己做了一些实验，和网上有大佬的说法结合，放在前后的差别并不大，我做的实验是语音唤醒，非 CV 实验。但是一般使用的时候都是放在前面的。

## Weight Normalization（WN）

- [ ] TODO

## Layer Normalization（LN）

- [ ] TODO

## Instance Normalization（IN）

- [ ] TODO

## Group Normalization（GN）

- [ ] TODO

### BN、LN、WN、IN 和 GN 的区别

**主要且常用**的归一化操作有** BN，LN，IN，GN**，示意图如图所示。

![](https://files.mdnice.com/user/6935/7842ce3e-a60f-4c37-b520-98851537f16d.png)

图中的蓝色部分，表示需要归一化的部分。其中两维$C$和$N$分别表示$channel$和$batch$  $size$，第三维表示$H$,$W$，可以理解为该维度大小是$H*W$，也就是拉长成一维，这样总体就可以用三维图形来表示。可以看出$BN$的计算和$batch$  $size$相关（蓝色区域为计算均值和方差的单元），而$LN$、$BN$和$GN$的计算和$batch$ $size$无关。同时$LN$和$IN$都可以看作是$GN$的特殊情况（$LN$是$group$=1 时候的$GN$，$IN$是$group=C$时候的$GN$）。

**参考资料**

- [深度学习中的五种归一化（BN、LN、IN、GN 和 SN）方法简介](https://blog.csdn.net/u013289254/article/details/99690730)

- [BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm 总结](https://blog.csdn.net/liuxiao214/article/details/81037416)

## 优化算法

- 随机梯度下降（SGD）
- Mini-Batch
- 动量（Momentum）
- Nesterov 动量
- AdaGrad
- AdaDelta
- RMSProp
- Adam
- Adamax
- Nadam
- [AMSGrad](http://ruder.io/optimizing-gradient-descent/index.html#amsgrad)
- AdaBound

**参考资料**

- [《Deep Learning》第八章：深度模型中的优化](https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/)

- [从 SGD 到 Adam —— 深度学习优化算法概览（一）](https://zhuanlan.zhihu.com/p/32626442)
- [Adam 究竟还有什么问题 —— 深度学习优化算法概览（二）](https://zhuanlan.zhihu.com/p/37269222)
- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)

## 011 梯度下降法
梯度下降的**本质**：是一种使用梯度去迭代更新权重参数使目标函数最小化的方法。

缺点：
1. 不保证全局最优
2. 到极小值附近收敛速度变慢
3. 可能出现“之”字型下降

- [温故知新——梯度下降（Gradient Descent）](https://zhuanlan.zhihu.com/p/72374201)
=======
### 梯度下降法

- [ ] TODO
### mini-batch 梯度下降法

小批量梯度下降算法是折中方案，选取训练集中一个小批量样本（一般是 2 的倍数，如 32，64,128 等）计算，这样可以保证训练过程更稳定，而且采用批量训练方法也可以利用矩阵计算的优势。这是目前最常用的梯度下降算法。

> 批量梯度下降算法  
> 对于批量梯度下降算法，其损失函数是在整个训练集上计算的，如果数据集比较大，可能会面临内存不足问题，而且其收敛速度一般比较慢。

区别点在于：**整个训练集**做一次损失函数计算还是**一个小批量样本**做一次损失函数计算

### 随机梯度下降法（SGD）

#### SGD 每步做什么，为什么能 online learning？

随机梯度下降算法是另外一个极端，损失函数是针对训练集中的一个训练样本计算的，又称为在线学习，即得到了一个样本，就可以执行一次参数更新。所以其收敛速度会快一些，但是有可能出现目标函数值震荡现象，因为高频率的参数更新导致了高方差。

区别点在于：**训练集中的一个训练样本**做一次损失函数计算

### 动量梯度下降法（Momentum）

引入一个指数加权平均的知识点。也就是下图中的前两行公式。使用这个公式，可以将之前的 dW 和 db 都联系起来，不再是每一次梯度都是独立的情况。
![Momentum](https://img-blog.csdn.net/20170912094835664?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWlucnVpeWFuZzk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

---

**参考资料**

- [简述动量 Momentum 梯度下降](https://blog.csdn.net/yinruiyang94/article/details/77944338)

### RMSprop

$$
S_{dW}=\beta S_{dW}+\left ( 1-\beta  \right )dW^{2}
$$

$$
S_{db}=\beta S_{db}+\left ( 1-\beta  \right )db^{2}
$$

$$
W=W-\alpha\frac{dW}{\sqrt{S_{dW}}}, b=b-\alpha\frac{db}{\sqrt{S_{db}}}
$$

更新权重的时候，使用除根号的方法，可以使较大的梯度大幅度变小，而较小的梯度小幅度变小，这样就可以使较大梯度方向上的波动小下来，那么整个梯度下降的过程中摆动就会比较小

### Adagrad

- [ ] TODO

### Adam

Adam 算法结合了 Momentum 和 RMSprop 梯度下降法，是一种极其常见的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。
$$
v_{dW}=\beta_{1} v_{dW}+\left ( 1-\beta_{1}  \right )dW
$$

$$
v_{db}=\beta_{1} v_{db}+\left ( 1-\beta_{1}  \right )db
$$

$$
S_{dW}=\beta_{2} S_{dW}+\left ( 1-\beta_{2}  \right )dW^{2}
$$

$$
S_{db}=\beta_{2} S_{db}+\left ( 1-\beta_{2}  \right )db^{2}
$$

$$
v_{dW}^{corrected}=\frac{v_{dW}}{1-\beta_{1}^{t}}
$$

$$
v_{db}^{corrected}=\frac{v_{db}}{1-\beta_{1}^{t}}
$$

$$
S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_{2}^{t}}
$$

$$
S_{db}^{corrected}=\frac{S_{db}}{1-\beta_{2}^{t}}
$$

$$
W:=W-\frac{av_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon }
$$

超参数：
$$
\alpha ,\beta _{1},\beta_{2},\varepsilon
$$

t 是迭代次数

- [ ] TODO 

## 017 常见的激活函数及其特点
- Sigmod
=======
#### Adam 优化器的迭代公式

- [ ] TODO

## 激活函数

- Tanh

- ReLU

---

- [常见的激活函数总结](https://blog.csdn.net/colourful_sky/article/details/79164720)

## 018 ReLU 及其变体
- Leaky ReLU
=======
**参考资料**

- [What is activate function?](https://yogayu.github.io/DeepLearningCourse/03/ActivateFunction.html)
- [资源 | 从 ReLU 到 Sinc，26 种神经网络激活函数可视化](https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ)

### Sigmoid

- [ ] TODO

#### Sigmoid 用作激活函数时，分类为什么要用交叉熵损失，而不用均方损失？

- [ ] TODO

### tanh

- [ ] TODO

### ReLU

- [ ] TODO

ReLU 相关变体

#### ReLU 激活函数为什么比 sigmoid 和 tanh 好？ 
- PReLU

- RReLU

- ELU

![ELU](https://pic1.zhimg.com/80/v2-b3946769cc26e4bba1937d24f72643c0_hd.jpg)
![](https://pic2.zhimg.com/80/v2-c637e0650621777690bd1897bf58c02d_hd.jpg)
---

- [激活函数综述](https://zhuanlan.zhihu.com/p/56210226)

#### ReLU 激活函数为什么能解决梯度消失问题？

- [ ] TODO

#### ReLU 有哪些变体？

- [ ] TODO

## Dropout

### Dropout 基本原理

- [ ] TODO

**参考资料**

- [理解 dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)

### Dropout 如何实现？

- [ ] TODO

### Drop 在训练和测试的区别

- [ ] TODO

## 损失函数（Loss）

- [ ] TODO

### Cross Entropy Loss（CE）

- [ ] TODO

### Hinge Loss

- [ ] TODO

### Focal Loss

- [ ] TODO

## 1*1 卷积有什么作用？

- [ ] TODO

## AlexNet

- 使用 ReLU 激活函数
- Dropout
- 数据增广

先给出 AlexNet 的一些参数和结构图： 

卷积层：5 层 

全连接层：3 层 

深度：8 层 

参数个数：60M 

神经元个数：650k 

分类数目：1000 类

![AlexNet](https://img-blog.csdn.net/20170516212827402?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvenlxZHJhZ29u/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**参考资料**

[AlexNet](https://dgschwend.github.io/netscope/#/preset/alexnet)

## VGG

**《Very Deep Convolutional Networks for Large-Scale Image Recognition》**

- arXiv：https://arxiv.org/abs/1409.1556
- intro：ICLR 2015
- homepage：http://www.robots.ox.ac.uk/~vgg/research/very_deep/

[VGG](https://arxiv.org/abs/1409.1556) 是 Oxford 的** V**isual **G**eometry **G**roup 的组提出的（大家应该能看出 VGG 名字的由来了）。该网络是在 ILSVRC 2014 上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。VGG 有两种结构，分别是 VGG16 和 VGG19，两者并没有本质上的区别，只是网络深度不一样。

VGG16 相比 AlexNet 的一个改进是采用**连续的几个 3x3 的卷积核代替 AlexNet 中的较大卷积核（11x11，7x7，5x5）**。对于给定的感受野（与输出有关的输入图片的局部大小），**采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）**。

简单来说，在 VGG 中，使用了 3 个 3x3 卷积核来代替 7x7 卷积核，使用了 2 个 3x3 卷积核来代替 5*5 卷积核，这样做的主要目的是在保证具有**相同感知野的条件下，提升了网络的深度**，在一定程度上提升了神经网络的效果。

比如，3 个步长为 1 的 3x3 卷积核的一层层叠加作用可看成一个大小为 7 的感受野（其实就表示 3 个 3x3 连续卷积相当于一个 7x7 卷积），其参数总量为 3x(9xC^2) ，如果直接使用 7x7 卷积核，其参数总量为 49xC^2 ，这里 C 指的是输入和输出的通道数。很明显，27xC^2 小于 49xC^2，即减少了参数；而且 3x3 卷积核有利于更好地保持图像性质。

这里解释一下为什么使用 2 个 3x3 卷积核可以来代替 5*5 卷积核：

5x5 卷积看做一个小的全连接网络在 5x5 区域滑动，我们可以先用一个 3x3 的卷积滤波器卷积，然后再用一个全连接层连接这个 3x3 卷积输出，这个全连接层我们也可以看做一个 3x3 卷积层。这样我们就可以用两个 3x3 卷积级联（叠加）起来代替一个 5x5 卷积。

具体如下图所示：

![](imgs/DLIB-0012.png)

至于为什么使用 3 个 3x3 卷积核可以来代替 7*7 卷积核，推导过程与上述类似，大家可以自行绘图理解。

下面是 VGG 网络的结构（VGG16 和 VGG19 都在）：

![VGG](https://d2mxuefqeaa7sj.cloudfront.net/s_8C760A111A4204FB24FFC30E04E069BD755C4EEFD62ACBA4B54BBA2A78E13E8C_1491022251600_VGGNet.png)

- VGG16 包含了 16 个隐藏层（13 个卷积层和 3 个全连接层），如上图中的 D 列所示
- VGG19 包含了 19 个隐藏层（16 个卷积层和 3 个全连接层），如上图中的 E 列所示

VGG 网络的结构非常一致，从头到尾全部使用的是 3x3 的卷积和 2x2 的 max pooling。

如果你想看到更加形象化的 VGG 网络，可以使用 [经典卷积神经网络（CNN）结构可视化工具](https://mp.weixin.qq.com/s/gktWxh1p2rR2Jz-A7rs_UQ) 来查看高清无码的 [VGG 网络](https://dgschwend.github.io/netscope/#/preset/vgg-16)。

**VGG 优点：**

- VGGNet 的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。
- 几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5 或 7x7）卷积层好：
- 验证了通过不断加深网络结构可以提升性能。

**VGG 缺点**：

VGG 耗费更多计算资源，并且使用了更多的参数（这里不是 3x3 卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG 可是有 3 个全连接层啊！

PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。

注：很多 pretrained 的方法就是使用 VGG 的 model（主要是 16 和 19），VGG 相对其他的方法，参数空间很大，最终的 model 有 500 多 m，AlexNet 只有 200m，GoogLeNet 更少，所以 train 一个 vgg 模型通常要花费更长的时间，所幸有公开的 pretrained model 让我们很方便的使用。

关于感受野：

假设你一层一层地重叠了 3 个 3x3 的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个 3x3 的视野。

**代码篇：VGG 训练与测试**

这里推荐两个开源库，训练请参考 [tensorflow-vgg](https://github.com/machrisaa/tensorflow-vgg)，快速测试请参考 [VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)。

代码我就不介绍了，其实跟上述内容一致，跟着原理看 code 应该会很快。我快速跑了一下 [VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)，代码亲测可用，效果很 nice，就是 model 下载比较烦。

贴心的 Amusi 已经为你准备好了 [VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/) 的测试代码、model 和图像。需要的同学可以关注 CVer 微信公众号，后台回复：VGG。

天道酬勤，还有很多知识要学，想想都刺激~Fighting！

**参考资料**

- [《Very Deep Convolutional Networks for Large-Scale Image Recognition》](https://arxiv.org/abs/1409.1556)
- [深度网络 VGG 理解](https://blog.csdn.net/wcy12341189/article/details/56281618)

- [深度学习经典卷积神经网络之 VGGNet](https://blog.csdn.net/marsjhao/article/details/72955935)
- [VGG16 结构可视化](https://dgschwend.github.io/netscope/#/preset/vgg-16)

- [tensorflow-vgg](https://github.com/machrisaa/tensorflow-vgg)

- [VGG-in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)

- [机器学习进阶笔记之五 | 深入理解 VGG\Residual Network](https://zhuanlan.zhihu.com/p/23518167)

## ResNet

**1.ResNet 意义**

随着网络的加深，出现了训练集准确率下降的现象，我们可以确定这不是由于 Overfit 过拟合造成的（过拟合的情况训练集应该准确率很高）；所以作者针对这个问题提出了一种全新的网络，叫深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如图 1； 
![](https://img-blog.csdn.net/20180114184946861?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGFucmFuMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
*图 1 Shortcut Connection*

这里问大家一个问题，残差指的是什么？ 

其中 ResNet 提出了两种 mapping：

一种是 identity mapping，指的就是图 1 中”弯弯的曲线”，另一种 residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x 

identity mapping 顾名思义，就是指本身，也就是公式中的 x，而 residual mapping 指的是“差”，也就是 y−x，所以残差指的就是 F(x) 部分。 

为什么 ResNet 可以解决“随着网络加深，准确率不下降”的问题？ 

理论上，对于“随着网络加深，准确率下降”的问题，Resnet 提供了两种选择方式，也就是 identity mapping 和 residual mapping，如果网络已经到达最优，继续加深网络，residual mapping 将被 push 为 0，只剩下 identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。

**2.ResNet 结构**

它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut 就是“抄近道”的意思，看下图我们就能大致理解： 

![image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803221515.png)

**ResNet 的 F(x) 究竟长什么样子？**

残差的思想都是去掉相同的主体部分，从而突出微小的变化

> F 是求和前网络映射，H 是从输入到求和后的网络映射。比如把 5 映射到 5.1，那么引入残差前是 F'(5)=5.1，引入残差后是 H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的 F'和 F 都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如 s 输出从 5.1 变到 5.2，映射 F'的输出增加了 1/51=2%，而对于残差结构输出从 5.1 到 5.2，映射 F 是从 0.1 到 0.2，增加了 100%。明显后者输出变化对权重的调整作用更大，所以效果更好。

关于 H(x) F(x) x 三者之间的关系：

> 网络输入是 x，网络的输出是 F(x)，网络要拟合的目标是 H(x)，  
传统网络的训练目标是 F(x)=H(x)。  
残差网络，则是把传统网络的输出 F(x) 处理一下，加上输入，将 F(x)+x 作为最终的输出，训练目标是 F(x)=H(x)-x，因此得名残差网络。  
现在我们要训练一个深层的网络，它可能过深，假设存在一个性能最强的完美网络 N，与它相比我们的网络中必定有一些层是多余的，那么这些多余的层的训练目标是恒等变换，只有达到这个目标我们的网络性能才能跟 N 一样。  
对于这些需要实现恒等变换的多余的层，要拟合的目标就成了 H(x)=x，在传统网络中，网络的输出目标是 F(x)=x，这比较困难，而在残差网络中，拟合的目标成了 x-x=0，网络的输出目标为 F(x)=0，这比前者要容易得多。
> 
> 作者：王殊
> 链接：https://www.zhihu.com/question/53224378/answer/343061012

**参考资料**

- [ResNet 解析](https://blog.csdn.net/lanran2/article/details/79057994)
- [ResNet 论文笔记](https://blog.csdn.net/wspba/article/details/56019373)

- [残差网络 ResNet 笔记](https://www.jianshu.com/p/e58437f39f65)

- [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)     

- [译文](https://www.jianshu.com/p/46d76bd56766)
- [Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image Classification: From Microsoft to Facebook [Part 1]](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624)
- [给妹纸的深度学习教学 (4)——同 Residual 玩耍](https://zhuanlan.zhihu.com/p/28413039)
- [Residual Networks 理解](https://zhuanlan.zhihu.com/p/32173684)
- [Identity Mapping in ResNet](https://zhuanlan.zhihu.com/p/32206896)
- [resnet（残差网络）的 F（x）究竟长什么样子？](https://www.zhihu.com/question/53224378)
- [ResNet 到底在解决一个什么问题呢？](https://www.zhihu.com/question/64494691)

### ResNet 为什么不用 Dropout?

### **YOLOv1**

(1) 给一个输入图像，首先将图像划分成 7 * 7 的网格。

- [ ] TODO

**参考资料**

- <https://www.zhihu.com/question/325139089>
- https://zhuanlan.zhihu.com/p/60923972

### 为什么 ResNet 不在一开始就使用 residual block, 而是使用一个 7×7 的卷积？

先上一下 paper 里的图例：
YOLO 检测网络包含 24 个卷积层（用来提取特征）和 2 个全联接层（用来预测图像位置和类类别置信度），并且使用了大量的 1x1 的卷积用来降低上一层的 layer 到下一层的特征空间。

![](https://img-blog.csdnimg.cn/20190328144834122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dieXk0MjI5OQ==,size_16,color_FFFFFF,t_70)

YOLO 将输入图像分成 SxS 个格子，每个格子负责检测‘落入’该格子的物体。若某个物体的中心位置的坐标落入到某个格子，那么这个格子就负责检测出这个物体。如下图所示，图中物体狗的中心点（红色原点）落入第 5 行、第 2 列的格子内，所以这个格子负责预测图像中的物体狗。

![](https://img-blog.csdnimg.cn/20190328144901578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dieXk0MjI5OQ==,size_16,color_FFFFFF,t_70)

![](https://img-blog.csdnimg.cn/20181204184148966.png)

这样的结果是针对每一个 bbox 而言的目标类别概率分布的置信值，这个置信值同时表达了 2 样东西，一个是目标是某个类别的概率，一个是预测的 bbox 离真实的 bbox 有多远。

![](https://img-blog.csdnimg.cn/20190328150725398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dieXk0MjI5OQ==,size_16,color_FFFFFF,t_70)

总结：

YOLO v1 也有和当年其它杰出的目标检测系统做对比，但在今天来看，这个并不十分重要，重要的是我们需要理解 YOLO 快的原因。

 - YOLO 就是一个撒渔网的捕鱼过程，一次性搞定所有的目标定位。

 - YOLO 快的原因在于比较粗的粒度将输入图片划分网格，然后做预测。

 - YOLO 的算法精髓都体现在它的 Loss 设计上及作者如何针对问题改进 Loss，这种思考问题的方式才是最值得我们学习的。

缺馅：

 - 输入尺寸固定：由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率。其它分辨率需要缩放成改分辨率。

 - 占比较小的目标检测效果不好。虽然每个格子可以预测 B 个 bounding box，但是最终只选择只选择 IOU 最高的 bounding box 作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。

**参考资料**

[YOLO V1 全网最详细的解读](https://blog.csdn.net/gbyy42299/article/details/88869766)

[目标检测算法 YOLO 最耐心细致的讲解](https://blog.csdn.net/briblue/article/details/84794511 )

[yolo 回归型的物体检测](https://blog.csdn.net/Jinlong_Xu/article/details/77888100)

[目标检测网络之 YOLOv3](https://www.cnblogs.com/makefile/p/YOLOv3.html)
---

NMS 图解 (IOU 略过）：

NMS 主要就是通过迭代的形式，不断的以最大得分的框去与其他框做 IoU 操作，并过滤那些 IoU 较大（即交集较大）的框。如图 3 图 4 所示 NMS 的计算过程。

1、根据候选框的类别分类概率做排序，假如有 4 个 BBox ，其置信度 A>B>C>D。

2、先标记最大概率矩形框 A 是算法要保留的 BBox；

3、从最大概率矩形框 A 开始，分别判断 ABC 与 D 的重叠度 IOU（两框的交并比）是否大于某个设定的阈值 (0.5)，假设 D 与 A 的重叠度超过阈值，那么就舍弃 D；

4、从剩下的矩形框 BC 中，选择概率最大的 B，标记为保留，然后判读 C 与 B 的重叠度，扔掉重叠度超过设定阈值的矩形框；

5、一直重复进行，标记完所有要保留下来的矩形框。

![](https://pic1.zhimg.com/80/v2-30ea7fae78641826246a81a86baef220_hd.jpg)

<center>图 3 猫和狗两类目标检测</center>

![](https://pic2.zhimg.com/80/v2-2ff70f78e5da8fe5d1ad2724296d5c61_hd.jpg)

<center>图 4 NMS 算法过程</center>

---

### **YOLOv2**
=======
![img](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803221520.jpeg)

原因：7x7 卷积实际上是用来直接对**输入图片**降采样 (early downsampling), 注意像 7x7 这样的大卷积核一般只出现在** input layer**

**目的是：**  尽可能**保留原始图像的信息，** 而不需要增加 channels 数。

**本质上是：** 多 channels 的非线性激活层是非常昂贵的，在** input laye**r 用** big kernel **换多 channels 是划算的
![](https://upload-images.jianshu.io/upload_images/4155986-ca9655bb520bfccf?imageMogr2/auto-orient/)

>【Tips】BN 层的作用：
> （1）加速收敛   //数据同分布，缓解了 ICS 现象，并且可使用更大的学习率和更多的激活函数。  
> （2）控制过拟合，可以少用或不用 Dropout 和正则  //引用均值和方差，增加数据噪声，即数据增强  
> （3）允许使用较大的学习率  // 数据同分布  
> 在使用 BN 前，减小学习率、小心的权重初始化的目的是：使其输出的数据分布不要发生太大的变化。

High Resolution Classifier
=======
注意一下，resnet 接入 residual block 前 pixel 为 56x56 的 layer, channels 数才** 64**, 但是同样大小的 layer, 在 vgg-19 里已经有** 256 **个 channels 了。

这里要强调一下，只有在 input layer 层，也就是**最靠近输入图片**的那层，才用大卷积，原因如下：

深度学习领域，有一种广泛的直觉，即更大的卷积更好，但更昂贵。输入层中的特征数量 (224x224) 是如此之小（相对于隐藏层），第一卷积可以非常大而不会大幅增加实际的权重数。**如果你想在某个地方进行大卷积，第一层通常是唯一的选择**。

我认为神经网络的第一层是最基本的，因为它基本上只是将数据嵌入到一个新的更大的向量空间中。ResNet 在第二层之前没有开始其特征层跳过，所以看起来作者想要在开始整花里胡哨的 layers 之前尽可能保留图像里更多的 primary features.

题外话，同时期的 GoogLeNet 也在 input layer 用到了 7x7 大卷积，所以 resnet 作者的灵感来源于 GoogLeNet 也说不定，至于非要追问为啥这么用，也许最直接的理由就是"深度学习就像炼丹，因为这样网络工作得更好，所以作者就这么用了". 

之前的 YOLO 利用全连接层的数据完成边框的预测，导致丢失较多的空间信息，定位不准。作者在这一版本中借鉴了 Faster R-CNN 中的 anchor 思想，回顾一下，anchor 是 RNP 网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测 9 种不同大小的建议框。看到 YOLOv2 的这一借鉴，我只能说 SSD 的作者是有先见之明的。

为了引入 anchor boxes 来预测 bounding boxes，作者在网络中果断去掉了全连接层。剩下的具体怎么操作呢？首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为 416 * 416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个 center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个 cell 来预测这些物体的位置，否则就要用中间的 4 个 cell 来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2 使用了卷积层降采样（factor 为 32），使得输入卷积网络的 416 * 416 图片最终得到 13 * 13 的卷积特征图（416/32=13）。

再说个有趣的例子，resnet 模型是实验先于理论，实验证明有效，后面才陆续有人研究为啥有效，比如 [The Shattered Gradients Problem: If resnets are the answer, then what is the question?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.08591)  可不就是炼丹么？

**参考资料**

- [为什么 resnet 不在一开始就使用 residual block, 而是使用一个 7×7 的卷积？](https://www.zhihu.com/question/330735327/answer/725695411)

### 什么是 Bottlenet layer？

- [ ] TODO

### ResNet 如何解决梯度消失？

![](https://pic4.zhimg.com/80/v2-72c0ca9eafee4e818f55c464aa60e2ff_hd.jpg)
=======
- [ ] TODO

### ResNet 网络越来越深，准确率会不会提升？
- [ ] TODO

## ResNet v2

![image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803221710.png)

---

**参考资料**

- [《Identity Mappings in Deep Residual Networks》](https://arxiv.org/abs/1603.05027)
- [Feature Extractor[ResNet v2]](https://www.cnblogs.com/shouhuxianjian/p/7770658.html)
- [ResNetV2：ResNet 深度解析](https://blog.csdn.net/lanran2/article/details/80247515)
- [ResNet v2 论文笔记](https://blog.csdn.net/u014061630/article/details/80558661)
- [[ResNet 系] 002 ResNet-v2](https://segmentfault.com/a/1190000011228906)

### ResNet v1 与 ResNet v2 的区别

​	ResNet V2 和 ResNet V1 的主要区别在于，作者通过研究 ResNet 残差学习单元的传播公式，发现前馈和反馈信息可以直接传输，因此 skip  connection 的非线性激活函数（如 ReLU）替换为 Identity Mappings(y = x)。同时，ResNet V2 在每一层中都使用了 Batch Normalization。这样处理之后，新的残差学习单元将比以前更容易训练且泛化性更强。

- [tensorflow 学习笔记——ResNet](https://www.cnblogs.com/wj-1314/p/11519663.html)

### ResNet v2 的 ReLU 激活函数有什么不同？

- [ ] TODO

## ResNeXt

在`ResNet`提出`deeper`可以带来网络性质提高的同时，`WideResNet`则认为`Wider`也可以带来深度网络性能的改善。为了打破或`deeper`，或`wider`的常规思路，ResNeXt 则认为可以引入一个新维度，称之为`cardinality`。

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803222454.webp)

- [ResNeXt 算法详解](https://blog.csdn.net/u014380165/article/details/71667916)
- ResNet 的其他变种：WRNS(Wide Residual Networks)、Res2Net、ReXNet

## Inception 系列（V1-V4）

### InceptionV1

- [ ] TODO

### InceptionV2

- [ ] TODO
  [YOLO 升级版：YOLOv2 和 YOLO9000 解析](https://zhuanlan.zhihu.com/p/25052190)

## 023 R-CNN 系列
### InceptionV3

- [ ] TODO

### InceptionV4

- [ ] TODO

**参考资料**

- [一文概览 Inception 家族的「奋斗史」](https://baijiahao.baidu.com/s?id=1601882944953788623&wfr=spider&for=pc)
- [inception-v1,v2,v3,v4----论文笔记](https://blog.csdn.net/weixin_39953502/article/details/80966046)

## DenseNet

在以往的网络都是从

- 要么深（比如`ResNet`，解决了网络深时候的梯度消失问题）
- 要么宽（比如`GoogleNet`的`Inception`）的网络，

而作者则是从`feature`入手，通过对`feature`的极致利用达到更好的效果和更少的参数。

`DenseNet`网络有以下优点：

- 由于密集连接方式，DenseNet 提升了梯度的反向传播，**使得网络更容易训练**。
- **参数更小且计算更高效**，这有点违反直觉，由于 DenseNet 是通过 concat 特征来实现短路连接，实现了特征重用，并且采用较小的 growth rate，每个层所独有的特征图是比较小的；
- 由于**特征复用**，最后的分类器使用了低级特征。

为了解决随着网络深度的增加，网络梯度消失的问题，在`ResNet`网络 之后，科研界把研究重心放在通过更有效的跳跃连接的方法上。`DenseNet`系列网络延续这个思路，并做到了一个极致，就是直接将所有层都连接起来。`DenseNet`层连接方法示意图如图所示。![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803222850.webp)

![Image](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20210803222924.webp)

### 为什么 DenseNet 比 ResNet 好？

- [ ] TODO

### 为什么 DenseNet 比 ResNet 更耗显存？

- [ ] TODO

## SE-Net

- [ ] TODO

### Squeeze-Excitation 结构是怎么实现的？

TODO

## FCN

一句话概括就是：FCN 将传统网络后面的全连接层换成了卷积层，这样网络输出不再是类别而是 heatmap；同时为了解决因为卷积和池化对图像尺寸的影响，提出使用上采样的方式恢复。

作者的 FCN 主要使用了三种技术：

- 卷积化（Convolutional）

- 上采样（Upsample）
- 跳跃结构（Skip Layer）

卷积化

卷积化即是将普通的分类网络，比如 VGG16，ResNet50/101 等网络丢弃全连接层，换上对应的卷积层即可。

上采样

此处的上采样即是反卷积（Deconvolution）。当然关于这个名字不同框架不同，Caffe 和 Kera 里叫 Deconvolution，而 tensorflow 里叫 conv_transpose。CS231n 这门课中说，叫 conv_transpose 更为合适。

众所诸知，普通的池化（为什么这儿是普通的池化请看后文）会缩小图片的尺寸，比如 VGG16 五次池化后图片被缩小了 32 倍。为了得到和原图等大的分割图，我们需要上采样/反卷积。

反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和后向传播，只用颠倒卷积的前后向传播即可。所以无论优化还是后向传播算法都是没有问题。

跳跃结构（Skip Layers）

（这个奇怪的名字是我翻译的，好像一般叫忽略连接结构）这个结构的作用就在于优化结果，因为如果将全卷积之后的结果直接上采样得到的结果是很粗糙的，所以作者将不同池化层的结果进行上采样之后来优化输出。

上采样获得与输入一样的尺寸
文章采用的网络经过 5 次卷积+池化后，图像尺寸依次缩小了 2、4、8、16、32 倍，对最后一层做 32 倍上采样，就可以得到与原图一样的大小

作者发现，仅对第 5 层做 32 倍反卷积（deconvolution），得到的结果不太精确。于是将第 4 层和第 3 层的输出也依次反卷积（图５）

**参考资料**

[【总结】图像语义分割之 FCN 和 CRF](https://zhuanlan.zhihu.com/p/22308032)

[图像语义分割（1）- FCN](https://blog.csdn.net/zizi7/article/details/77093447)

[全卷积网络 FCN 详解](https://www.cnblogs.com/gujianhan/p/6030639.html)

## U-Net

本文介绍一种编码器-解码器结构。编码器逐渐减少池化层的空间维度，解码器逐步修复物体的细节和空间维度。编码器和解码器之间通常存在快捷连接，因此能帮助解码器更好地修复目标的细节。U-Net 是这种方法中最常用的结构。

fcn(fully convolutional natwork) 的思想是：修改一个普通的逐层收缩的网络，用上采样 (up sampling)(？？反卷积）操作代替网络后部的池化 (pooling) 操作。因此，这些层增加了输出的分辨率。为了使用局部的信息，在网络收缩过程（路径）中产生的高分辨率特征 (high resolution features) ，被连接到了修改后网络的上采样的结果上。在此之后，一个卷积层基于这些信息综合得到更精确的结果。

与 fcn(fully convolutional natwork) 不同的是，我们的网络在上采样部分依然有大量的特征通道 (feature channels)，这使得网络可以将环境信息向更高的分辨率层 (higher resolution layers) 传播。结果是，扩张路径基本对称于收缩路径。网络不存在任何全连接层 (fully connected layers)，并且，只使用每个卷积的有效部分，例如，分割图 (segmentation map) 只包含这样一些像素点，这些像素点的完整上下文都出现在输入图像中。为了预测图像边界区域的像素点，我们采用镜像图像的方式补全缺失的环境像素。这个 tiling 方法在使用网络分割大图像时是非常有用的，因为如果不这么做，GPU 显存会限制图像分辨率。
我们的训练数据太少，因此我们采用弹性形变的方式增加数据。这可以让模型学习得到形变不变性。这对医学图像分割是非常重要的，因为组织的形变是非常常见的情况，并且计算机可以很有效的模拟真实的形变。在 [3] 中指出了在无监督特征学习中，增加数据以获取不变性的重要性。

**参考资料**

- [U-net 翻译](https://blog.csdn.net/natsuka/article/details/78565229)

## DeepLab 系列

- [ ] TODO

**参考资料**

- [Semantic Segmentation --DeepLab(1,2,3) 系列总结](https://blog.csdn.net/u011974639/article/details/79148719)

## 边框回顾（Bounding-Box Regression）

如下图所示，绿色的框表示真实值 Ground Truth, 红色的框为 Selective Search 提取的候选区域/框 Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准 (IoU<0.5)， 这张图也相当于没有正确的检测出飞机。

![](https://www.julyedu.com/Public/Image/Question/1525499418_635.png)

如果我们能对红色的框进行微调 fine-tuning，使得经过微调后的窗口跟 Ground Truth 更接近， 这样岂不是定位会更准确。 而 Bounding-box regression 就是用来微调这个窗口的。

边框回归是什么？

对于窗口一般使用四维向量 (x,y,w,h)(x,y,w,h) 来表示， 分别表示窗口的中心点坐标和宽高。 对于图 2, 红色的框 P 代表原始的 Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口 G^。

![](https://www.julyedu.com/Public/Image/Question/1525499529_241.png)

所以，边框回归的目的即是：给定 (Px,Py,Pw,Ph) 寻找一种映射 f， 使得 f(Px,Py,Pw,Ph)=(Gx^,Gy^,Gw^,Gh^) 并且 (Gx^,Gy^,Gw^,Gh^)≈(Gx,Gy,Gw,Gh)

边框回归怎么做的？

那么经过何种变换才能从图 2 中的窗口 P 变为窗口 G^呢？ 比较简单的思路就是：平移+尺度放缩

先做平移 (Δx,Δy)，Δx=Pwdx(P),Δy=Phdy(P) 这是 R-CNN 论文的：
G^x=Pwdx(P)+Px,(1)
G^y=Phdy(P)+Py,(2)

然后再做尺度缩放 (Sw,Sh), Sw=exp(dw(P)),Sh=exp(dh(P)), 对应论文中：
G^w=Pwexp(dw(P)),(3)
G^h=Phexp(dh(P)),(4)

观察 (1)-(4) 我们发现， 边框回归学习就是 dx(P),dy(P),dw(P),dh(P) 这四个变换。

下一步就是设计算法那得到这四个映射。

线性回归就是给定输入的特征向量 X, 学习一组参数 W, 使得经过线性回归后的值跟真实值 Y(Ground Truth) 非常接近。即 Y≈WX。 那么 Bounding-box 中我们的输入以及输出分别是什么呢？

Input:
RegionProposal→P=(Px,Py,Pw,Ph) 这个是什么？ 输入就是这四个数值吗？其实真正的输入是这个窗口对应的 CNN 特征，也就是 R-CNN 中的 Pool5 feature（特征向量）。 （注：训练阶段输入还包括 Ground Truth， 也就是下边提到的 t∗=(tx,ty,tw,th))

Output:
需要进行的平移变换和尺度缩放 dx(P),dy(P),dw(P),dh(P)，或者说是Δx,Δy,Sw,Sh。我们的最终输出不应该是 Ground Truth 吗？ 是的， 但是有了这四个变换我们就可以直接得到 Ground Truth。

这里还有个问题， 根据 (1)~(4) 我们可以知道， P 经过 dx(P),dy(P),dw(P),dh(P) 得到的并不是真实值 G，而是预测值 G^。的确，这四个值应该是经过 Ground Truth 和 Proposal 计算得到的真正需要的平移量 (tx,ty) 和尺度缩放 (tw,th)。 

这也就是 R-CNN 中的 (6)~(9)： 
tx=(Gx−Px)/Pw,(6)

ty=(Gy−Py)/Ph,(7)

tw=log(Gw/Pw),(8)

th=log(Gh/Ph),(9)

那么目标函数可以表示为 d∗(P)=wT∗Φ5(P)，Φ5(P) 是输入 Proposal 的特征向量，w∗是要学习的参数（*表示 x,y,w,h， 也就是每一个变换对应一个目标函数） , d∗(P) 是得到的预测值。

我们要让预测值跟真实值 t∗=(tx,ty,tw,th) 差距最小， 得到损失函数为：
Loss=∑iN(ti∗−w^T∗ϕ5(Pi))2

函数优化目标为：

W∗=argminw∗∑iN(ti∗−w^T∗ϕ5(Pi))2+λ||w^∗||2

利用梯度下降法或者最小二乘法就可以得到 w∗。

**参考资料**

- [bounding box regression](http://caffecn.cn/?/question/160)
- [边框回归 (Bounding Box Regression) 详解](https://blog.csdn.net/zijin0802034/article/details/77685438)

- [什么是边框回归 Bounding-Box regression，以及为什么要做、怎么做](https://www.julyedu.com/question/big/kp_id/26/ques_id/2139)

## 反卷积（deconv）/转置卷积（trans）

**参考资料**

- [反卷积 (Deconvolution)、上采样 (UNSampling) 与上池化 (UnPooling)](https://blog.csdn.net/a_a_ron/article/details/79181108)
- [Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)

## 空洞卷积（dilated/Atrous conv）

- [ ] TODO

**参考资料**

- [如何理解空洞卷积（dilated convolution）？](<https://www.zhihu.com/question/54149221>)

## Pooling 层原理

- [ ] TODO

## depthwise 卷积加速比推导

- [ ] TODO

## 为什么降采用使用 max pooling，而分类使用 average pooling

- [ ] TODO

## max pooling 如何反向传播

- [ ] TODO

## 反卷积

TODO

## 组卷积（group convolution）

- [ ] TODO

在说明分组卷积之前我们用一张图来体会一下一般的卷积操作。 

![常规卷积操作。png](imgs/DLIB-0014.png)

从上图可以看出，一般的卷积会对输入数据的整体一起做卷积操作，即输入数据：H1×W1×C1；而卷积核大小为 h1×w1，通道为 C1，一共有 C2 个，然后卷积得到的输出数据就是 H2×W2×C2。这里我们假设输出和输出的分辨率是不变的。主要看这个过程是一气呵成的，这对于存储器的容量提出了更高的要求。 

但是分组卷积明显就没有那么多的参数。先用图片直观地感受一下分组卷积的过程。对于上面所说的同样的一个问题，分组卷积就如下图所示。 

![组卷积操作。png](imgs/DLIB-0015.png)

可以看到，图中将输入数据分成了 2 组（组数为 g），需要注意的是，这种分组只是在深度上进行划分，即某几个通道编为一组，这个具体的数量由（C1/g）决定。因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了（C1/g），而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了（C2/g）个，而不是原来的 C2 了。然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用 concatenate 的方式组合起来，最终的输出数据的通道仍旧是 C2。也就是说，分组数 g 决定以后，那么我们将并行的运算 g 个相同的卷积过程，每个过程里（每组），输入数据为 H1×W1×C1/g，卷积核大小为 h1×w1×C1/g，一共有 C2/g 个，输出数据为 H2×W2×C2/g。

举个例子：

Group conv 本身就极大地减少了参数。比如当输入通道为 256，输出通道也为 256，kernel size 为 3×3，不做 Group conv 参数为 256×3×3×256。实施分组卷积时，若 group 为 8，每个 group 的 input channel 和 output channel 均为 32，参数为 8×32×3×3×32，是原来的八分之一。而 Group conv 最后每一组输出的 feature maps 应该是以 concatenate 的方式组合。 
Alex 认为 group conv 的方式能够增加 filter 之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。

**参考资料**

- [A Tutorial on Filter Groups (Grouped Convolution)](https://blog.yani.io/filter-group-tutorial/)

- [深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)

## 交错组卷积（Interleaved group convolutions，IGC）

**参考资料**

- [学界 | MSRA 王井东详解 ICCV 2017 入选论文：通用卷积神经网络交错组卷积](https://www.sohu.com/a/161110049_465975)
- [视频：基于交错组卷积的高效深度神经网络](https://edu.csdn.net/course/play/8320/171433?s=1)

## 空洞/扩张卷积（Dilated/Atrous Convolution）

Dilated convolution/Atrous convolution 可以叫空洞卷积或者扩张卷积。

背景：语义分割中 pooling 和 up-sampling layer 层。pooling 会降低图像尺寸的同时增大感受野，而 up-sampling 操作扩大图像尺寸，这样虽然恢复了大小，但很多细节被池化操作丢失了。

需求：能不能设计一种新的操作，不通过 pooling 也能有较大的感受野看到更多的信息呢？

目的：替代 pooling 和 up-sampling 运算，既增大感受野又不减小图像大小。

简述：在标准的 convolution map 里注入空洞，以此来增加 reception field。相比原来的正常 convolution，dilated convolution 多了一个 hyper-parameter 称之为 dilation rate 指的是 kernel 的间隔数量 (e.g. 正常的 convolution 是 dilatation rate 1)。

空洞卷积诞生于图像分割领域，图像输入到网络中经过 CNN 提取特征，再经过 pooling 降低图像尺度的同时增大感受野。由于图像分割是 pixel−wise 预测输出，所以还需要通过 upsampling 将变小的图像恢复到原始大小。upsampling 通常是通过 deconv（转置卷积）完成。因此图像分割 FCN 有两个关键步骤：池化操作增大感受野，upsampling 操作扩大图像尺寸。这儿有个问题，就是虽然图像经过 upsampling 操作恢复了大小，但是很多细节还是被池化操作丢失了。那么有没有办法既增大了感受野又不减小图像大小呢？Dilated conv 横空出世。

![image.png](imgs/DLIB-0016.png)

注意事项：

1. 为什么不直接使用 5x5 或者 7x7 的卷积核？这不也增加了感受野么？

答：增大卷积核能增大感受野，但是只是线性增长，参考答案里的那个公式，(kernel-1)*layer，并不能达到空洞卷积的指数增长。

2.2-dilated 要在 1-dilated 的基础上才能达到 7 的感受野（如上图 a、b 所示）

关于空洞卷积的另一种概括：

Dilated Convolution 问题的引出，是因为 down-sample 之后的为了让 input 和 output 的尺寸一致。我们需要 up-sample，但是 up-sample 会丢失信息。如果不采用 pooling，就无需下采样和上采样步骤了。但是这样会导致 kernel 的感受野变小，导致预测不精确。如果采用大的 kernel 话，一来训练的参数变大。二来没有小的 kernel 叠加的正则作用，所以 kernel size 变大行不通。

由此 Dilated Convolution 是在不改变 kernel size 的条件下，增大感受野。

**参考资料**

- [《Multi-Scale Context Aggregation by Dilated Convolutions》](https://arxiv.org/abs/1511.07122) 
- [《Rethinking Atrous Convolution for Semantic Image Segmentation》](https://arxiv.org/abs/1706.05587)

- [如何理解空洞卷积（dilated convolution）？](https://www.zhihu.com/question/54149221)

- [Dilated/Atrous conv 空洞卷积/多孔卷积](https://blog.csdn.net/silence2015/article/details/79748729)

- [Multi-Scale Context Aggregation by Dilated Convolution 对空洞卷积（扩张卷积）、感受野的理解](https://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&fps=1)

- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)

- [tf.nn.atrous_conv2d](https://tensorflow.google.cn/api_docs/python/tf/nn/atrous_conv2d)

## 转置卷积（Transposed Convolutions/deconvlution）

转置卷积（transposed Convolutions）又名反卷积（deconvolution）或是分数步长卷积（fractially straced convolutions）。反卷积（Transposed Convolution, Fractionally Strided Convolution or Deconvolution）的概念第一次出现是 Zeiler 在 2010 年发表的论文 Deconvolutional networks 中。

**转置卷积和反卷积的区别**

那什么是反卷积？从字面上理解就是卷积的逆过程。值得注意的反卷积虽然存在，但是在深度学习中并不常用。而转置卷积虽然又名反卷积，却不是真正意义上的反卷积。因为根据反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号。而事实是，转置卷积只能还原 shape 大小，而不能还原 value。你可以理解成，至少在数值方面上，转置卷积不能实现卷积操作的逆过程。所以说转置卷积与真正的反卷积有点相似，因为两者产生了相同的空间分辨率。但是又名反卷积（deconvolutions）的这种叫法是不合适的，因为它不符合反卷积的概念。

简单来说，转置矩阵就是一种上采样过程。

正常卷积过程如下，利用 3x3 的卷积核对 4x4 的输入进行卷积，输出结果为 2x2

![卷积过程](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif?raw=true)

转置卷积过程如下，利用 3x3 的卷积核对"做了补 0"的 2x2 输入进行卷积，输出结果为 4x4。

![转置卷积](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides_transposed.gif?raw=true)

上述的卷积运算和转置卷积是"尺寸"对应的，卷积的输入大小与转置卷积的输出大小一致，分别可以看成下采样和上采样操作。

**参考资料**

- [Transposed Convolution, Fractionally Strided Convolution or Deconvolution](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/)
- [深度学习 | 反卷积/转置卷积 的理解 transposed conv/deconv](https://blog.csdn.net/u014722627/article/details/60574260)

## Group Normalization

- [ ] 

## Xception

- [ ] TODO

## SENet

**SENet**

论文：《Squeeze-and-Excitation Networks》 

论文链接：https://arxiv.org/abs/1709.01507 

代码地址：https://github.com/hujie-frank/SENet

论文的动机是从特征通道之间的关系入手，希望显式地建模特征通道之间的相互依赖关系。另外，没有引入一个新的空间维度来进行特征通道间的融合，而是采用了一种全新的“特征重标定”策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去增强有用的特征并抑制对当前任务用处不大的特征，通俗来讲，就是让网络利用全局信息有选择的增强有益 feature 通道并抑制无用 feature 通道，从而能实现 feature 通道自适应校准。 

![Schema of SE-Inception and SE-ResNet modules](imgs/DLIB-0017.png)

**参考资料**

- [SENet 学习笔记](https://blog.csdn.net/xjz18298268521/article/details/79078551)

## SKNet

- [ ] TODO

**参考资料**

- [SKNet——SENet 孪生兄弟篇](https://zhuanlan.zhihu.com/p/59690223)
- [后 ResNet 时代：SENet 与 SKNet](https://zhuanlan.zhihu.com/p/60187262)

## GCNet

- [ ] TODO

**参考资料**

- [GCNet：当 Non-local 遇见 SENet](https://zhuanlan.zhihu.com/p/64988633)
- [2019 GCNet（attention 机制，目标检测 backbone 性能提升）论文阅读笔记](https://zhuanlan.zhihu.com/p/65776424)

## Octave Convolution

- [ ] TODO

**参考资料**

- [如何评价最新的 Octave Convolution？](https://www.zhihu.com/question/320462422/)

## MobileNet 系列（V1-V3）

- [ ] TODO

### MobileNetV1

**参考资料**

- [深度解读谷歌 MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)

### MobileNetV2

- [ ] TODO

### MobileNetV3

- [ ] TODO

- [如何评价 google Searching for MobileNetV3？](https://www.zhihu.com/question/323419310)

### MobileNet 系列为什么快？各有多少层？多少参数？

- [ ] TODO

### MobileNetV1、MobileNetV2 和 MobileNetV3 有什么区别

MobileNetv1：在 depthwise separable convolutions（参考 Xception）方法的基础上提供了高校模型设计的两个选择：宽度因子（width multiplie）和分辨率因子（resolution multiplier）。深度可分离卷积 depthwise separable convolutions（参考 Xception）的本质是冗余信息更小的稀疏化表达。

下面介绍两幅 Xception 中 depthwise separable convolution 的图示：

![image.png](imgs/DLIB-0018.png)

![image.png](imgs/DLIB-0019.png)

深度可分离卷积的过程是①用 16 个 3×3 大小的卷积核（1 通道）分别与输入的 16 通道的数据做卷积（这里使用了 16 个 1 通道的卷积核，输入数据的每个通道用 1 个 3×3 的卷积核卷积），得到了 16 个通道的特征图，我们说该步操作是 depthwise（逐层）的，在叠加 16 个特征图之前，②接着用 32 个 1×1 大小的卷积核（16 通道）在这 16 个特征图进行卷积运算，将 16 个通道的信息进行融合（用 1×1 的卷积进行不同通道间的信息融合），我们说该步操作是 pointwise（逐像素）的。这样我们可以算出整个过程使用了 3×3×16+（1×1×16）×32 =656 个参数。

注：上述描述与标准的卷积非常的不同，第一点在于使用非 1x1 卷积核时，是单 channel 的（可以说是 1 通道），即上一层输出的每个 channel 都有与之对应的卷积核。而标准的卷积过程，卷积核是多 channel 的。第二点在于使用 1x1 卷积核实现多 channel 的融合，并利用多个 1x1 卷积核生成多 channel。表达的可能不是很清楚，但结合图示其实就容易明白了。

一般卷积核的 channel 也常称为深度（depth），所以叫做深度可分离，即原来为多 channel 组合，现在变成了单 channel 分离。

**参考资料**

- [深度解读谷歌 MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)
- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)

### MobileNetv2 为什么会加 shotcut？

- [ ] TODO

### MobileNet V2 中的 Residual 结构最先是哪个网络提出来的？

- [ ] TODO

## ShuffleNet 系列（V1-V2++）

- [ ] TODO

### ShuffleNetV1

- [ ] TODO

- [轻量级网络--ShuffleNet 论文解读](https://blog.csdn.net/u011974639/article/details/79200559)
- [轻量级网络 ShuffleNet v1](https://www.jianshu.com/p/29f4ec483b96)
- [CNN 模型之 ShuffleNet](https://zhuanlan.zhihu.com/p/32304419)

### ShuffleNetV2

- [ ] TODO

**参考资料**

- [ShuffleNetV2：轻量级 CNN 网络中的桂冠](https://zhuanlan.zhihu.com/p/48261931)
- [轻量级神经网络“巡礼”（一）—— ShuffleNetV2](https://zhuanlan.zhihu.com/p/67009992)
- [ShufflenetV2_高效网络的 4 条实用准则](https://zhuanlan.zhihu.com/p/42288448)
- [ShuffNet v1 和 ShuffleNet v2](https://zhuanlan.zhihu.com/p/51566209)

## IGC 系列（V1-V3）

- [ ] TODO

**参考资料**

- [微软资深研究员详解基于交错组卷积的高效 DNN | 公开课笔记](https://mp.weixin.qq.com/s/ZLIL9A3RS0jj8knbXP9uFQ)

## 深度可分离网络（Depth separable convolution）

- [ ] TODO

## 学习率如何调整

- [ ] TODO

## 神经网络的深度和宽度作用

- [ ] TODO

## 网络压缩与量化

- [ ] TODO

**参考资料**

- [网络压缩-量化方法对比](https://blog.csdn.net/shuzfan/article/details/51678499)

## Batch Size

- [ ] TODO

**参考资料**

- [怎么选取训练神经网络时的 Batch size?](https://www.zhihu.com/question/61607442)

- [谈谈深度学习中的 Batch_Size](https://blog.csdn.net/lien0906/article/details/79166196)

## BN 和 Dropout 在训练和测试时的差别

- [ ] TODO

**参考资料**

- [BN 和 Dropout 在训练和测试时的差别](https://zhuanlan.zhihu.com/p/61725100)

## 深度学习调参有哪些技巧？

**参考资料**

- <https://www.zhihu.com/question/25097993/answer/651617880>

## 为什么深度学习中的模型基本用 3x3 和 5x5 的卷积（奇数），而不是 2x2 和 4x4 的卷积（偶数）？

**参考资料**

- <https://www.zhihu.com/question/321773456>

## 深度学习训练中是否有必要使用 L1 获得稀疏解？

- [ ] TODO

**参考资料**

- <https://www.zhihu.com/question/51822759>

## EfficientNet

- [ ] TODO

**参考资料**

- [如何评价谷歌大脑的 EfficientNet？](https://www.zhihu.com/question/326833457)
- [EfficientNet-可能是迄今为止最好的 CNN 网络](https://zhuanlan.zhihu.com/p/67834114)
- [EfficientNet 论文解读](https://zhuanlan.zhihu.com/p/70369784)
- [EfficientNet：调参侠的福音（ICML 2019）](https://zhuanlan.zhihu.com/p/69349360)

## 如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？

BN 最早被认为通过降低所谓** Internal Covariate Shift**，这种想法的出处可考至 [Understanding the difficulty of training deep feedforward neural networks](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)，想必这也是 batch norm 作者这么设计的初衷。但是这种想法并没有过多实验支持，比如说去年 NeurlPS 这篇 paper 作者做了实验，在 batch norm 之后加上一些随机扰动（non-zero mean and non-unit variance，人为引入 covariate shift），发现效果仍然比不加好很多。为什么放在 batch norm layer 之后而不是之前？因为为了证伪 batch norm 通过 forward pass 这一步降低 covariate shift 来提升网络训练效率的。这样说来故事就变得很有趣了，也就是说我们大概都理解一些 BN 对 BN 层之前网络噪音的好处，那么能不能研究一下它对它后面 layer 的影响？所以这些研究从优化的角度，有如下几种观点。

1. BN 通过修改 loss function， 可以令 loss 的和 loss 的梯度均满足更强的 Lipschitzness 性质（即函数 f 满足 L-Lipschitz 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) -smooth，令 L 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) 更小，后者其实等同于 f Hessian 的 eigenvalue 小于 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta) ，可以作为光滑程度的度量，其实吧我觉得，一般 convex optimization 里拿这个度量算 convergence rate 是神器，对于 non-convex optimization，不懂鸭，paper 里好像也没写的样子），这么做的好处是当步子迈得大的时候，我们可以更自信地告诉自己计算出来的梯度可以更好地近似实际的梯度，因此也不容易让优化掉进小坑里。有意思的地方来了，是不是我在某些地方插入一个 1/1000 layer，把梯度的 L-Lipschitz 变成 1/1000L-Lipschitz 就能让函数优化的更好了呢？其实不是的，因为单纯除以函数会改变整个优化问题，而 BN 做了不仅仅 rescale 这件事情，还让原来近似最优的点在做完变化之后，仍然保留在原来不远的位置。这也就是这篇文章的核心论点，BN 做的是问题 reparametrization 而不是简单的 scaling。 [1]
2. BN 把优化这件事情分解成了优化参数的方向和长度两个任务，这么做呢可以解耦层与层之间的 dependency 因此会让 curvature 结构更易于优化。这篇证了 convergence rate，但由于没有认真读，所以感觉没太多资格评价。[2]

归一化手段是否殊途同归？很可能是的，在 [1] 的 3.3 作者也尝试了 Lp normalization，也得到了和 BN 差不多的效果。至于 Layer norm 还是 weight norm，可能都可以顺着这个思路进行研究鸭，无论是通过 [1] 还是 [2]，可能今年的 paper 里就见分晓了，let's see。

1. [How Does Batch Normalization Help Optimization?](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.11604) 
2. [Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.10694)

**参考资料**

- [如何理解归一化（Normalization）对于神经网络（深度学习）的帮助？](https://www.zhihu.com/question/326034346/answer/708331566)

## 多标签分类怎么解决？

- [ ] TODO

## TODO

​    
