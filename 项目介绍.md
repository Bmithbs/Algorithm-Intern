
# 基于卷积神经网络的遥感旋转目标检测

## 背景介绍

### 遥感场景下进行旋转目标检测的第一个难点：数据规模小。

因此要尽量增加**数据集的规模**、泛化性以及表征无人机影像特点的充分性，使得数据集在样本数量、贴近真实世界的复杂度上足以支撑神学学习方法。

数据增强分为有监督和无监督，有监督的数据增强方式是目标检测算法最常用的数据扩增方法。YOLOv5 中只用了有监督的数据增强方式，会对输入的图像依次进行以下数据增强操作：

Mosaic、Mixup、随机仿射变化，等？

Mosaic 的主要思想为：混合多张具有不同语义信息的图片，让检测器检测出超出常规语境的信息，增强模型的鲁棒性。其触发几率可以随机，图像的拼接方式可以区间内随机，图像的拼接图片可以随机选择。同时相当于变相增大一次训练的图片数量，另外由于 Mosaic 过强的随机性，与其他增强方案的组合可以有效防止过拟合，提高长时间训练的收益。

### 第二个难点：大中小目标分布不均；

遥感数据的特点：

1. 小目标数量众多，排列密集或稀疏，中大目标的数量较少
2. 小目标的分布均匀，中大目标相比而言分布不均
3. 由于遥感图像需要进行切割，因此会加剧目标分布不均的情况。

**Mosaic 的拼接特性能过极大程度上改善 DOTA 数据集目标分布不均的情况，并且由于其拼接的随机性，随着训练时间的加长，改善效果会更明显。**

### 第三个难点：目标尺度变化剧烈

大部分目标的尺度较小，提供的细节信息有限，且随着网络的加深，小目标的细节信息会逐渐丢失，因此我们**需要网络结构对小目标有一定敏感性，即较强的多尺度特征提取能力**。高空场景下，目标尺度时而密集，时而稀疏，模型需要满足单个图像中检测多个不同尺度多个对象的能力。

因此需要考虑选取合适的 featuremap 的结果（Backbone + Neck），既要保证丰富的浅层位置信息同时保证深层的语义信息。yolov5 的 SPP + PANet 对这个问题有很好的处理

## 旋转目标检测

原始YOLOv5项目的应用场景为自然场景下的目标，目标检测边框为水平矩形框（Horizontal Bounding Box，HBB），毕竟我们的视角就是水平视角。

在遥感场景下，很多目标的长宽比较大，且排列密集，因此遥感目标更适合利用旋转框进行检测。

损失计算：置信度损失、class分类损失、θ角度分类损失、bbox边框回归损失。

- class分类损失：无需更改，注意数据索引部分即可。
- θ角度分类损失：由于我们添加的θ是分类任务，照葫芦画瓢，添加分类损失就可以了，
- bbox边框回归损失：边框回归损失部分依旧采用IOU/GIOU/CIOU/DIOU损失函数。
- 置信度损失：置信度分支的权重系数依然选择水平边框的之间的IOU/GIOU/CIOU/DIOU；

## YOLOv5 的 head

Head 在训练时将 Anchor 铺设在不同尺度的 featuremap 上，通过与 GT_box 的匹配度进行筛选，完成不同尺度的 anchor 的正样本采样工作，之后进入损失函数训练。

YOLOv5 的采样策略：

- 将图像划分为 n×n 的网格，GT 中心落入哪个网格内，哪个网格内的 anchor 就负责匹配该 GT ，当然该网格内的 anchor 依旧需要进行正负样本的标定，但是匹配原则不以 IoU 为阈值，而是以 shape 阈值为匹配原则，即预设 Anchor 与 GT_box 计算宽高比，若宽高比例超出设定阈值范围（比如 1/4 ~ 4），则匹配失败，若处于设定阈值范围内，则匹配成功
- GT_box 不仅仅匹配中心所在网络的 Anchor，还会寻找最近的两个网格进行匹配，匹配原则同上。整体策略中，样本非负即正，没有忽略样本。


## 所做工作

1. 对数据进行处理
   1. 对标注格式的转换，将数据格式转换为YOLO的的方式
   2. 对数据进行裁剪
   3. 对裁剪后的数据进行清洗， 由于裁剪后的数据很多没有目标，因此遍历标签将不含有目标的数据进行剔除
2. 在进行训练的过程中，进行多尺度训练，按照两个不同的比例对图片进行裁剪，
3. 进行多尺度训练检测，在进行检测的过程中，进行多尺度检测，将图片以两种不同的比率进行裁剪，进行检测，map 上升了 3 个百分点，其中小目标（车辆）的 AP 上升了 8 个百分点
4. 调参，进行优化

## 最终的结果

达到了 68%的 mAP，但是其中的大目标，如足球场，桥梁等较大的目标 AP 较低，只有 50%左右，分析原因，一是在裁剪的过程中，将这些目标裁掉了，二是这些大目标在数据集中的数量占比较少，对 loss 的贡献较少。但是总的来说，这个结果还是可以接受的。
